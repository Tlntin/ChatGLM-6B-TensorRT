#!/usr/bin/env python3
# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31
# Generation Command: /home/tlntin/anaconda3/envs/torch/bin/polygraphy run ../output/onnx_output/chatglm_6b.onnx --onnxrt --trt --workspace 1000000000 --save-engine=../models/model-FP32-MarkAll.plan --atol 1e-3 --rtol 1e-3 --verbose --onnx-outputs mark all --trt-outputs mark all --trt-opt-shapes input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value:[0,1,32,128]  --input-shapes input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value:[0,1,32,128] --gen-script compare.py
# This script compares /home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx between ONNX-Runtime and TensorRT.

from polygraphy.logger import G_LOGGER
G_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}
from polygraphy import constants
from polygraphy.backend.common import SaveBytes
from polygraphy.backend.onnx import BytesFromOnnx, ModifyOutputs as ModifyOnnxOutputs, OnnxFromPath
from polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx
from polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner
from polygraphy.common import TensorMetadata
from polygraphy.comparator import Comparator, CompareFunc, DataLoader
from polygraphy.exception import PolygraphyException

# Data Loader
data_loader = DataLoader(input_metadata=TensorMetadata().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', None, [0, 1, 32, 128]))

# Loaders
load_onnx = OnnxFromPath('/home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx')
modify_outputs = ModifyOnnxOutputs(load_onnx, outputs=constants.MARK_ALL)
serialize_onnx = BytesFromOnnx(modify_outputs)
build_onnxrt_session = SessionFromOnnx(serialize_onnx)
parse_network_from_onnx = NetworkFromOnnxPath('/home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx')
set_network_outputs = ModifyNetworkOutputs(parse_network_from_onnx, outputs=constants.MARK_ALL)
profiles = [
    Profile().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', min=[0, 1, 32, 128], opt=[0, 1, 32, 128], max=[0, 1, 32, 128])
]
create_trt_config = CreateTrtConfig(max_workspace_size=1000000000, profiles=profiles)
build_engine = EngineBytesFromNetwork(set_network_outputs, config=create_trt_config)
save_engine_bytes = SaveBytes(build_engine, path='../models/model-FP32-MarkAll.plan')
deserialize_engine = EngineFromBytes(save_engine_bytes)

# Runners
runners = [
    OnnxrtRunner(build_onnxrt_session),
    TrtRunner(deserialize_engine),
]

# Runner Execution
results = Comparator.run(runners, data_loader=data_loader)

success = True
# Accuracy Comparison
compare_func = CompareFunc.simple(rtol={'': 0.001}, atol={'': 0.001})
success &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))

# Report Results
if not success:
    raise PolygraphyException('FAILED')
