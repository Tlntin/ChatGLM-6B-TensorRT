#!/usr/bin/env python3
# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31
# code gen with onnx_trt_compare.sh
# but i edit the code to make it more readable
import tensorrt as trt
import os
import numpy as np
import shutil
from polygraphy.logger import G_LOGGER
G_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}

from polygraphy import constants
import onnx
from polygraphy.backend.common import SaveBytes
from polygraphy.backend.onnx import modify_outputs, onnx_from_path
from polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx
from polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner
from polygraphy.common import TensorMetadata
from polygraphy.comparator import Comparator, CompareFunc, DataLoader
from polygraphy.exception import PolygraphyException
from polygraphy.backend.trt import network_from_onnx_path
from colored import fg, bg, stylize



# --dir info--
now_dir = os.path.dirname(os.path.abspath(__file__))
project_dir = os.path.dirname(now_dir)
import sys
sys.path.append(project_dir)
from tensorrt_export.onnx2trt_no_cache import (
    get_network_profiles,
    MyLogger,
    get_network_definition,
    time_cache_path
)



output_dir = os.path.join(project_dir,"output")
new_onnx_dir = os.path.join(output_dir, "onnx_output_new")
if not os.path.exists(new_onnx_dir):
    os.mkdir(new_onnx_dir)
else:
    for file in os.listdir(new_onnx_dir):
        os.remove(os.path.join(new_onnx_dir, file))
new_onnx_dir2 = os.path.join(output_dir, "onnx_output_new2")
if not os.path.exists(new_onnx_dir2):
    os.mkdir(new_onnx_dir2)
else:
    for file in os.listdir(new_onnx_dir2):
        os.remove(os.path.join(new_onnx_dir2, file))

onnx_path = os.path.join(output_dir, "onnx_output", "chatglm_6b.onnx")
new_onnx_path = os.path.join(new_onnx_dir, "chatglm_6b.onnx")
new_onnx_path2 = os.path.join(new_onnx_dir2, "chatglm_6b.onnx")
model_dir = os.path.join(project_dir, "models")
trt_model_path = os.path.join(model_dir, "model-FP16-MarkAll.plan")
use_time_cache = True
num_layers = 1


# Data Loader
data_loader = DataLoader(
    input_metadata=TensorMetadata()
    .add('input_ids', dtype=np.int64, shape=(1, 512), min_shape=None, max_shape=None)
    .add('position_ids', dtype=np.int64, shape=(1, 2, 512), min_shape=None, max_shape=None)
    .add('attention_mask', dtype=np.bool_, shape=(1,1, 512, 512), min_shape=None, max_shape=None)
    .add("past_key_values.0.decorder.key", dtype=np.float32, shape=(1, 1, 32, 128), min_shape=None, max_shape=None)
    .add("past_key_values.0.decorder.value", dtype=np.float32, shape=(1, 1, 32, 128), min_shape=None, max_shape=None)
)
# load onnx
print("loading onnx model from", onnx_path)
onnx_model = onnx_from_path(onnx_path)
input_list = onnx_model.graph.input
input_names = [i.name for i in input_list]
output_list = onnx_model.graph.output
output_names = [o.name for o in output_list]
exclude_outputs = input_names
# mark all layers as output for onnx model
# warning this will make the onnx model output all layers with no type and no shape
new_onnx_model = modify_outputs(
    model=onnx_model,
    outputs=constants.MARK_ALL,
    exclude_outputs=exclude_outputs,
)
new_output_list = new_onnx_model.graph.output
new_output_names = [o.name for o in new_output_list]
onnx_input_num = len(new_onnx_model.graph.input)
onnx_output_num = len(new_onnx_model.graph.output)
print("onnx input num:", onnx_input_num, "onnx output num:", onnx_output_num)
onnx.save_model(
    new_onnx_model,
    new_onnx_path,
    save_as_external_data=True,
    all_tensors_to_one_file=False
)
# load onnxrt
build_onnxrt_session = SessionFromOnnx(new_onnx_path)
# get onnxrt output tensor info(name, type, shape)
sess = build_onnxrt_session()
sess_output = sess.get_outputs()
for node in sess_output:
    if node.type == "tensor(float)":
        dtype = onnx.TensorProto.FLOAT
    elif node.type == "tensor(float16)":
        dtype = onnx.TensorProto.FLOAT16
    elif node.type == "tensor(bool)":
        dtype = onnx.TensorProto.BOOL
    elif node.type == "tensor(int64)":
        dtype = onnx.TensorProto.INT64
    else:
        print("unknown dtype:", node.type)
        raise ValueError
    output_tensor = onnx.helper.make_tensor_value_info(
        node.name, dtype, None
    )
    # replace the output tensor
    for i, vi in enumerate(new_onnx_model.graph.output):
        if vi.name == node.name:
            new_onnx_model.graph.output[i].CopyFrom(output_tensor)
# save again
for file in os.listdir(new_onnx_dir):
    shutil.copy(os.path.join(new_onnx_dir, file), new_onnx_dir2)
onnx.save_model(
    new_onnx_model,
    new_onnx_path2,
    save_as_external_data=True,
    all_tensors_to_one_file=False
)
print("===========onnx model loaded=========================")
# build trt engine
print("===========building trt engine=========================")
# prepare trt builder
builder = trt.Builder(MyLogger())
builder.max_threads = os.cpu_count() // 2
config = builder.create_builder_config()
# open fp16 mode
config.flags = 1 << int(trt.BuilderFlag.FP16)
# disable tf32 mode, it will use fp32 mode replace it;
config.flags = config.flags & ~( 1 << int(trt.BuilderFlag.TF32) )
# use obey precision constraints
config.flags = config.flags | (1 << int(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS))
profile_list = get_network_profiles(builder, num_layers=num_layers)
for profile in profile_list:
    config.add_optimization_profile(profile)

# use time cache
time_cache = b""
# read time cache
if use_time_cache:
    if os.path.exists(time_cache_path):
        time_cache = open(time_cache_path, "rb").read()
        if time_cache is None:
            time_cache = b""
            print(stylize("read time cache failed", fg("red")))
        else:
            print(
                stylize("read time cache from {}".format(time_cache_path),
                        fg("green")
                ))
    else:
        time_cache = b""
    # set time cache
    print("set time cache ok!")
    cache = config.create_timing_cache(time_cache)
    config.set_timing_cache(cache, True)
# _, network, _ = network_from_onnx_path(onnx_path)

_, network,_ = NetworkFromOnnxPath(new_onnx_path2)()
network = get_network_definition(network)
network_input_number = network.num_inputs
network_output_number = network.num_outputs
print("TensorRT input num:", network_input_number, "TensorRT output num:", network_output_number)
serialized_engine = builder.build_serialized_network(network, config)
if serialized_engine is None:
    raise Exception("trt engine build failed")
save_engine_bytes = SaveBytes(serialized_engine, path=trt_model_path)
deserialize_engine = EngineFromBytes(save_engine_bytes)
print("===========trt engine build OK=========================")
# save time cache
if use_time_cache and not os.path.exists(time_cache_path):
    time_cache = config.get_timing_cache()
    time_cache_data = time_cache.serialize()
    if time_cache is not None:
        open(time_cache_path, "wb").write(time_cache_data)
        print(stylize("save time cache to {}".format(time_cache_path), fg("green")))

# Runners
runners = [
    OnnxrtRunner(build_onnxrt_session),
    TrtRunner(deserialize_engine),
]

# Runner Execution
results = Comparator.run(runners, data_loader=data_loader)

success = True
# Accuracy Comparison
compare_func = CompareFunc.simple(rtol={'': 0.001}, atol={'': 0.001})
success &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))

# Report Results
if not success:
    raise PolygraphyException('FAILED')
